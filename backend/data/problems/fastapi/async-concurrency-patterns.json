{
  "id": "async-concurrency-patterns",
  "title": "Async Concurrency Patterns (Semaphore, Queue, Gather)",
  "category": "fastapi",
  "difficulty": "hard",
  "description": "Implement common **asyncio concurrency patterns** used in FastAPI applications for parallel execution, rate-controlled concurrency, and producer-consumer workflows. Since the sandbox runs synchronous code, you will implement synchronous simulations of these patterns.\n\nBuild the following:\n\n1. **`TaskPool`** — Simulates `asyncio.gather` with concurrency control (like `asyncio.Semaphore`):\n   - `__init__(self, max_concurrent: int = 0)` — 0 means unlimited concurrency.\n   - `submit(self, func: Callable, *args) -> str` — Register a task. Return task_id `\"task-{n}\"`.\n   - `run_all(self) -> list[dict]` — Execute all tasks. If `max_concurrent > 0`, process in batches of that size. Return `[{\"task_id\": str, \"result\": Any, \"error\": str | None, \"batch\": int}]`. `batch` is the batch number (starting from 1; unlimited = all batch 1).\n\n2. **`WorkQueue`** — Producer-consumer pattern:\n   - `__init__(self, max_size: int = 0)` — 0 means unlimited queue size.\n   - `put(self, item: Any) -> bool` — Add item to queue. Return `False` if queue is full.\n   - `get(self) -> Any` — Remove and return the oldest item. Raise `IndexError` if empty.\n   - `peek(self) -> Any` — Return oldest item without removing. Raise `IndexError` if empty.\n   - `size(self) -> int` — Return current queue size.\n   - `is_empty(self) -> bool` — Check if empty.\n   - `is_full(self) -> bool` — Check if at max_size (always False if max_size=0).\n   - `drain(self) -> list` — Remove and return all items in order.\n\n3. **`Pipeline`** — Chain of processing steps:\n   - `__init__(self)` — Initialize with empty step list.\n   - `add_step(self, name: str, func: Callable) -> 'Pipeline'` — Add a processing step. Return `self` for chaining.\n   - `process(self, data: Any) -> dict` — Run data through all steps in order. Each step's output is the next step's input. Return `{\"result\": final_output, \"steps\": [{\"name\": str, \"input\": Any, \"output\": Any}]}`.\n   - `process_batch(self, items: list) -> list[dict]` — Run `process` on each item. Return list of results.\n\n4. **`FanOutFanIn`** — Split work across workers, then merge:\n   - `__init__(self, workers: list[Callable], merger: Callable)` — Workers process data independently; merger combines results.\n   - `execute(self, data: Any) -> dict` — Send `data` to ALL workers. Collect results. Call `merger(results_list)`. Return `{\"worker_results\": [...], \"merged\": merged_value}`.\n\n5. **`Debouncer`** — Deduplicates rapid calls:\n   - `__init__(self, delay: float)` — Set debounce delay.\n   - `call(self, key: str, func: Callable) -> Any | None` — If `key` was called within `delay` seconds, skip (return `None`). Otherwise, execute and return result.\n   - `get_call_count(self, key: str) -> int` — Return how many times `key` was actually executed.\n\n6. **`Batcher`** — Accumulates items and processes in batches:\n   - `__init__(self, batch_size: int, processor: Callable)` — Set batch size and processor function.\n   - `add(self, item: Any) -> list | None` — Add item. If buffer reaches batch_size, process the batch and return result. Otherwise return `None`.\n   - `flush(self) -> list | None` — Process remaining items. Return result or `None` if empty.",
  "examples": [
    {
      "input": "pool = TaskPool(max_concurrent=2)\npool.submit(lambda: 'a')\npool.submit(lambda: 'b')\npool.submit(lambda: 'c')\nresults = pool.run_all()\n[(r['task_id'], r['batch']) for r in results]",
      "output": "[('task-1', 1), ('task-2', 1), ('task-3', 2)]",
      "explanation": "With max_concurrent=2, first two tasks in batch 1, third in batch 2."
    },
    {
      "input": "p = Pipeline()\np.add_step('double', lambda x: x * 2).add_step('add10', lambda x: x + 10)\np.process(5)['result']",
      "output": "20",
      "explanation": "5 → double → 10 → add10 → 20"
    }
  ],
  "constraints": [
    "TaskPool batches must respect max_concurrent (process in groups of that size).",
    "WorkQueue with max_size=0 has unlimited capacity.",
    "Pipeline.add_step must return self for method chaining.",
    "FanOutFanIn must pass the SAME data to all workers.",
    "Debouncer must use time.time() to track call timing.",
    "Batcher.flush must process remaining items even if buffer < batch_size."
  ],
  "starter_code": "import time\nfrom typing import Any, Callable\n\n\nclass TaskPool:\n    \"\"\"Simulates asyncio.gather with concurrency control.\"\"\"\n\n    def __init__(self, max_concurrent: int = 0):\n        pass\n\n    def submit(self, func: Callable, *args) -> str:\n        pass\n\n    def run_all(self) -> list[dict]:\n        pass\n\n\nclass WorkQueue:\n    \"\"\"Producer-consumer queue.\"\"\"\n\n    def __init__(self, max_size: int = 0):\n        pass\n\n    def put(self, item: Any) -> bool:\n        pass\n\n    def get(self) -> Any:\n        pass\n\n    def peek(self) -> Any:\n        pass\n\n    def size(self) -> int:\n        pass\n\n    def is_empty(self) -> bool:\n        pass\n\n    def is_full(self) -> bool:\n        pass\n\n    def drain(self) -> list:\n        pass\n\n\nclass Pipeline:\n    \"\"\"Chain of processing steps.\"\"\"\n\n    def __init__(self):\n        pass\n\n    def add_step(self, name: str, func: Callable) -> 'Pipeline':\n        pass\n\n    def process(self, data: Any) -> dict:\n        pass\n\n    def process_batch(self, items: list) -> list[dict]:\n        pass\n\n\nclass FanOutFanIn:\n    \"\"\"Split work across workers, then merge results.\"\"\"\n\n    def __init__(self, workers: list[Callable], merger: Callable):\n        pass\n\n    def execute(self, data: Any) -> dict:\n        pass\n\n\nclass Debouncer:\n    \"\"\"Deduplicates rapid calls within a delay window.\"\"\"\n\n    def __init__(self, delay: float):\n        pass\n\n    def call(self, key: str, func: Callable) -> Any | None:\n        pass\n\n    def get_call_count(self, key: str) -> int:\n        pass\n\n\nclass Batcher:\n    \"\"\"Accumulates items and processes in batches.\"\"\"\n\n    def __init__(self, batch_size: int, processor: Callable):\n        pass\n\n    def add(self, item: Any) -> list | None:\n        pass\n\n    def flush(self) -> list | None:\n        pass\n",
  "test_cases": [
    {
      "input": "pool = TaskPool(max_concurrent=2)\npool.submit(lambda: 1)\npool.submit(lambda: 2)\npool.submit(lambda: 3)\nresults = pool.run_all()\n[(r['result'], r['batch']) for r in results]",
      "expected": "[(1, 1), (2, 1), (3, 2)]",
      "is_hidden": false
    },
    {
      "input": "q = WorkQueue(max_size=2)\n(q.put('a'), q.put('b'), q.put('c'), q.size())",
      "expected": "(True, True, False, 2)",
      "is_hidden": false
    },
    {
      "input": "p = Pipeline()\np.add_step('upper', lambda s: s.upper()).add_step('exclaim', lambda s: s + '!')\nr = p.process('hello')\n(r['result'], len(r['steps']), r['steps'][0]['output'])",
      "expected": "('HELLO!', 2, 'HELLO')",
      "is_hidden": false
    },
    {
      "input": "fan = FanOutFanIn(\n    workers=[lambda x: x*2, lambda x: x*3, lambda x: x+10],\n    merger=lambda results: sum(results)\n)\nr = fan.execute(5)\n(sorted(r['worker_results']), r['merged'])",
      "expected": "([10, 15, 15], 40)",
      "is_hidden": true
    },
    {
      "input": "q = WorkQueue()\nfor i in range(5): q.put(i)\n(q.peek(), q.get(), q.get(), q.drain())",
      "expected": "(0, 0, 1, [2, 3, 4])",
      "is_hidden": true
    },
    {
      "input": "d = Debouncer(delay=0.1)\nr1 = d.call('key1', lambda: 'first')\nr2 = d.call('key1', lambda: 'second')\nimport time; time.sleep(0.15)\nr3 = d.call('key1', lambda: 'third')\n(r1, r2, r3, d.get_call_count('key1'))",
      "expected": "('first', None, 'third', 2)",
      "is_hidden": true
    },
    {
      "input": "b = Batcher(batch_size=3, processor=lambda items: [x*2 for x in items])\nr1 = b.add(1)\nr2 = b.add(2)\nr3 = b.add(3)\nr4 = b.add(4)\nflush = b.flush()\n(r1, r2, r3, r4, flush)",
      "expected": "(None, None, [2, 4, 6], None, [8])",
      "is_hidden": true
    },
    {
      "input": "pool = TaskPool()\nfor i in range(4): pool.submit(lambda i=i: i*10)\nresults = pool.run_all()\n([r['result'] for r in results], all(r['batch'] == 1 for r in results))",
      "expected": "([0, 10, 20, 30], True)",
      "is_hidden": true
    }
  ],
  "time_limit_minutes": 40,
  "tags": ["fastapi", "async", "concurrency", "producer-consumer", "pipeline"]
}
